import { JSONSchema7 } from 'json-schema';

/**
A tool has a name, a description, and a set of parameters.

Note: this is **not** the user-facing tool definition. The AI SDK methods will
map the user-facing tool definitions to this format.
 */
type LanguageModelV1FunctionTool = {
    /**
  The type of the tool. Only functions for now, but this gives us room to
  add more specific tool types in the future and use a discriminated union.
     */
    type: 'function';
    /**
  The name of the tool. Unique within this model call.
     */
    name: string;
    description?: string;
    parameters: JSONSchema7;
};

type LanguageModelV1CallSettings = {
    /**
     * Maximum number of tokens to generate.
     */
    maxTokens?: number;
    /**
     * Temperature setting. This is a number between 0 (almost no randomness) and
     * 1 (very random).
     *
     * Different LLM providers have different temperature
     * scales, so they'd need to map it (without mapping, the same temperature has
     * different effects on different models). The provider can also chose to map
     * this to topP, potentially even using a custom setting on their model.
     *
     * Note: This is an example of a setting that requires a clear specification of
     * the semantics.
     */
    temperature?: number;
    /**
     * Nucleus sampling. This is a number between 0 and 1.
     *
     * E.g. 0.1 would mean that only tokens with the top 10% probability mass
     * are considered.
     *
     * It is recommended to set either `temperature` or `topP`, but not both.
     */
    topP?: number;
    /**
     * Presence penalty setting. It affects the likelihood of the model to
     * repeat information that is already in the prompt.
     *
     * The presence penalty is a number between -1 (increase repetition)
     * and 1 (maximum penalty, decrease repetition). 0 means no penalty.
     */
    presencePenalty?: number;
    /**
     * Frequency penalty setting. It affects the likelihood of the model
     * to repeatedly use the same words or phrases.
     *
     * The frequency penalty is a number between -1 (increase repetition)
     * and 1 (maximum penalty, decrease repetition). 0 means no penalty.
     */
    frequencyPenalty?: number;
    /**
     * The seed (integer) to use for random sampling. If set and supported
     * by the model, calls will generate deterministic results.
     */
    seed?: number;
    /**
     * Abort signal for cancelling the operation.
     */
    abortSignal?: AbortSignal;
};

/**
 * A prompt is a list of messages.
 *
 * Note: Not all models and prompt formats support multi-modal inputs and
 * tool calls. The validation happens at runtime.
 *
 * Note: This is not a user-facing prompt. The AI SDK methods will map the
 * user-facing prompt types such as chat or instruction prompts to this format.
 */
type LanguageModelV1Prompt = Array<LanguageModelV1Message>;
type LanguageModelV1Message = {
    role: 'system';
    content: string;
} | {
    role: 'user';
    content: Array<LanguageModelV1TextPart | LanguageModelV1ImagePart>;
} | {
    role: 'assistant';
    content: Array<LanguageModelV1TextPart | LanguageModelV1ToolCallPart>;
} | {
    role: 'tool';
    content: Array<LanguageModelV1ToolResultPart>;
};
interface LanguageModelV1TextPart {
    type: 'text';
    /**
     * The text content.
     */
    text: string;
}
interface LanguageModelV1ImagePart {
    type: 'image';
    /**
     * Image data as a Uint8Array (e.g. from a Blob or Buffer) or a URL.
     */
    image: Uint8Array | URL;
    /**
     * Optional mime type of the image.
     */
    mimeType?: string;
}
interface LanguageModelV1ToolCallPart {
    type: 'tool-call';
    toolCallId: string;
    toolName: string;
    args: unknown;
}
interface LanguageModelV1ToolResultPart {
    type: 'tool-result';
    toolCallId: string;
    toolName: string;
    result: unknown;
}

type LanguageModelV1CallOptions = LanguageModelV1CallSettings & {
    /**
     * Whether the user provided the input as messages or as
     * a prompt. This can help guide non-chat models in the
     * expansion, bc different expansions can be needed for
     * chat/non-chat use cases.
     */
    inputFormat: 'messages' | 'prompt';
    /**
     * The mode affects the behavior of the language model. It is required to
     * support provider-independent streaming and generation of structured objects.
     * The model can take this information and e.g. configure json mode, the correct
     * low level grammar, etc. It can also be used to optimize the efficiency of the
     * streaming, e.g. tool-delta stream parts are only needed in the
     * object-tool mode.
     */
    mode: {
        type: 'regular';
        tools?: Array<LanguageModelV1FunctionTool>;
    } | {
        type: 'object-json';
    } | {
        type: 'object-grammar';
        schema: JSONSchema7;
    } | {
        type: 'object-tool';
        tool: LanguageModelV1FunctionTool;
    };
    /**
     * A language mode prompt is a standardized prompt type.
     *
     * Note: This is **not** the user-facing prompt. The AI SDK methods will map the
     * user-facing prompt types such as chat or instruction prompts to this format.
     * That approach allows us to evolve the user  facing prompts without breaking
     * the language model interface.
     */
    prompt: LanguageModelV1Prompt;
};

/**
 * Warning from the model provider for this call. The call will proceed, but e.g.
 * some settings might not be supported, which can lead to suboptimal results.
 */
type LanguageModelV1CallWarning = {
    type: 'unsupported-setting';
    setting: keyof LanguageModelV1CallSettings;
} | {
    type: 'other';
    message: string;
};

type LanguageModelV1FinishReason = 'stop' | 'length' | 'content-filter' | 'tool-calls' | 'error' | 'other';

type LanguageModelV1FunctionToolCall = {
    toolCallType: 'function';
    toolCallId: string;
    toolName: string;
    /**
     * Stringified JSON object with the tool call arguments. Must match the
     * parameters schema of the tool.
     */
    args: string;
};

/**
 * Experimental: Specification for a language model that implements the language model
 * interface version 1.
 */
type LanguageModelV1 = {
    /**
     * The language model must specify which language model interface
     * version it implements. This will allow us to evolve the language
     * model interface and retain backwards compatibility. The different
     * implementation versions can be handled as a discriminated union
     * on our side.
     */
    readonly specificationVersion: 'v1';
    /**
     * Name of the provider for logging purposes.
     */
    readonly provider: string;
    /**
     * Provider-specific model ID for logging purposes.
     */
    readonly modelId: string;
    /**
     * Default object generation mode that should be used with this model when
     * no mode is specified. Should be the mode with the best results for this
     * model. `undefined` can be returned if object generation is not supported.
     *
     * This is needed to generate the best objects possible w/o requiring the
     * user to explicitly specify the object generation mode.
     */
    readonly defaultObjectGenerationMode: 'json' | 'tool' | 'grammar' | undefined;
    /**
     * Generates a language model output (non-streaming).
     *
     * Naming: "do" prefix to prevent accidental direct usage of the method
     * by the user.
     */
    doGenerate(options: LanguageModelV1CallOptions): PromiseLike<{
        /**
         * Text that the model has generated. Can be undefined if the model
         * has only generated tool calls.
         */
        text?: string;
        /**
         * Tool calls that the model has generated. Can be undefined if the
         * model has only generated text.
         */
        toolCalls?: Array<LanguageModelV1FunctionToolCall>;
        /**
         * Finish reason.
         */
        finishReason: LanguageModelV1FinishReason;
        /**
         * Usage information.
         */
        usage: {
            promptTokens: number;
            completionTokens: number;
        };
        /**
         * Raw prompt and setting information for observability provider integration.
         */
        rawCall: {
            /**
             * Raw prompt after expansion and conversion to the format that the
             * provider uses to send the information to their API.
             */
            rawPrompt: unknown;
            /**
             * Raw settings that are used for the API call. Includes provider-specific
             * settings.
             */
            rawSettings: Record<string, unknown>;
        };
        warnings?: LanguageModelV1CallWarning[];
    }>;
    /**
     * Generates a language model output (streaming).
     *
     * Naming: "do" prefix to prevent accidental direct usage of the method
     * by the user.
     *
     * @return A stream of higher-level language model output parts.
     */
    doStream(options: LanguageModelV1CallOptions): PromiseLike<{
        stream: ReadableStream<LanguageModelV1StreamPart>;
        /**
         * Raw prompt and setting information for observability provider integration.
         */
        rawCall: {
            /**
             * Raw prompt after expansion and conversion to the format that the
             * provider uses to send the information to their API.
             */
            rawPrompt: unknown;
            /**
             * Raw settings that are used for the API call. Includes provider-specific
             * settings.
             */
            rawSettings: Record<string, unknown>;
        };
        warnings?: LanguageModelV1CallWarning[];
    }>;
};
type LanguageModelV1StreamPart = {
    type: 'text-delta';
    textDelta: string;
} | ({
    type: 'tool-call';
} & LanguageModelV1FunctionToolCall) | {
    type: 'tool-call-delta';
    toolCallType: 'function';
    toolCallId: string;
    toolName: string;
    argsTextDelta: string;
} | {
    type: 'finish';
    finishReason: LanguageModelV1FinishReason;
    usage: {
        promptTokens: number;
        completionTokens: number;
    };
} | {
    type: 'error';
    error: unknown;
};

type OpenAIChatModelId = 'gpt-4' | 'gpt-4-0314' | 'gpt-4-0613' | 'gpt-4-turbo-preview' | 'gpt-4-1106-preview' | 'gpt-4-0125-preview' | 'gpt-4-vision-preview' | 'gpt-4-32k' | 'gpt-4-32k-0314' | 'gpt-4-32k-0613' | 'gpt-3.5-turbo' | 'gpt-3.5-turbo-0125' | 'gpt-3.5-turbo-1106' | 'gpt-3.5-turbo-0301' | 'gpt-3.5-turbo-0613' | 'gpt-3.5-turbo-16k' | 'gpt-3.5-turbo-16k-0613' | (string & {});
interface OpenAIChatSettings {
    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     *
     * Accepts a JSON object that maps tokens (specified by their token ID in
     * the GPT tokenizer) to an associated bias value from -100 to 100. You
     * can use this tokenizer tool to convert text to token IDs. Mathematically,
     * the bias is added to the logits generated by the model prior to sampling.
     * The exact effect will vary per model, but values between -1 and 1 should
     * decrease or increase likelihood of selection; values like -100 or 100
     * should result in a ban or exclusive selection of the relevant token.
     *
     * As an example, you can pass {"50256": -100} to prevent the <|endoftext|>
     * token from being generated.
     */
    logitBias?: Record<number, number>;
    /**
     * A unique identifier representing your end-user, which can help OpenAI to
     * monitor and detect abuse. Learn more.
     */
    user?: string;
}

type OpenAIChatConfig = {
    provider: string;
    baseUrl: string;
    headers: () => Record<string, string | undefined>;
};
declare class OpenAIChatLanguageModel implements LanguageModelV1 {
    readonly specificationVersion = "v1";
    readonly defaultObjectGenerationMode = "tool";
    readonly modelId: OpenAIChatModelId;
    readonly settings: OpenAIChatSettings;
    private readonly config;
    constructor(modelId: OpenAIChatModelId, settings: OpenAIChatSettings, config: OpenAIChatConfig);
    get provider(): string;
    private getArgs;
    doGenerate(options: Parameters<LanguageModelV1['doGenerate']>[0]): Promise<Awaited<ReturnType<LanguageModelV1['doGenerate']>>>;
    doStream(options: Parameters<LanguageModelV1['doStream']>[0]): Promise<Awaited<ReturnType<LanguageModelV1['doStream']>>>;
}

type OpenAICompletionModelId = 'gpt-3.5-turbo-instruct' | (string & {});
interface OpenAICompletionSettings {
    /**
     * Echo back the prompt in addition to the completion
     */
    echo?: boolean;
    /**
     * Modify the likelihood of specified tokens appearing in the completion.
     *
     * Accepts a JSON object that maps tokens (specified by their token ID in
     * the GPT tokenizer) to an associated bias value from -100 to 100. You
     * can use this tokenizer tool to convert text to token IDs. Mathematically,
     * the bias is added to the logits generated by the model prior to sampling.
     * The exact effect will vary per model, but values between -1 and 1 should
     * decrease or increase likelihood of selection; values like -100 or 100
     * should result in a ban or exclusive selection of the relevant token.
     *
     * As an example, you can pass {"50256": -100} to prevent the <|endoftext|>
     * token from being generated.
     */
    logitBias?: Record<number, number>;
    /**
     * The suffix that comes after a completion of inserted text.
     */
    suffix?: string;
    /**
     * A unique identifier representing your end-user, which can help OpenAI to
     * monitor and detect abuse. Learn more.
     */
    user?: string;
}

type OpenAICompletionConfig = {
    provider: string;
    baseUrl: string;
    headers: () => Record<string, string | undefined>;
};
declare class OpenAICompletionLanguageModel implements LanguageModelV1 {
    readonly specificationVersion = "v1";
    readonly defaultObjectGenerationMode: undefined;
    readonly modelId: OpenAICompletionModelId;
    readonly settings: OpenAICompletionSettings;
    private readonly config;
    constructor(modelId: OpenAICompletionModelId, settings: OpenAICompletionSettings, config: OpenAICompletionConfig);
    get provider(): string;
    private getArgs;
    doGenerate(options: Parameters<LanguageModelV1['doGenerate']>[0]): Promise<Awaited<ReturnType<LanguageModelV1['doGenerate']>>>;
    doStream(options: Parameters<LanguageModelV1['doStream']>[0]): Promise<Awaited<ReturnType<LanguageModelV1['doStream']>>>;
}

/**
 * OpenAI provider.
 */
declare class OpenAI {
    readonly baseUrl?: string;
    readonly apiKey?: string;
    readonly organization?: string;
    constructor(options?: {
        baseUrl?: string;
        apiKey?: string;
        organization?: string;
    });
    private get baseConfig();
    chat(modelId: OpenAIChatModelId, settings?: OpenAIChatSettings): OpenAIChatLanguageModel;
    completion(modelId: OpenAICompletionModelId, settings?: OpenAICompletionSettings): OpenAICompletionLanguageModel;
}
/**
 * Default OpenAI provider instance.
 */
declare const openai: OpenAI;

export { OpenAI, openai };
